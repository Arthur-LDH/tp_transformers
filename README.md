*# tp_transformers

## Contexte du TP

Ce projet s'inscrit dans le cadre d'un Travail Pratique (TP) ax√© sur l'exploration et l'application des mod√®les de Transformers. L'objectif principal est de d√©velopper une solution de bout en bout, comprenant typiquement :
*   La s√©lection et l'exp√©rimentation avec un mod√®le de Transformer adapt√© √† une t√¢che sp√©cifique (par exemple, classification de texte, g√©n√©ration de texte, question-r√©ponse).
*   Le fine-tuning (ou l'entra√Ænement) du mod√®le sur un jeu de donn√©es pertinent.
*   L'exposition des fonctionnalit√©s du mod√®le entra√Æn√© via une API RESTful.
*   La cr√©ation d'une interface utilisateur simple (frontend) pour interagir avec le mod√®le via l'API.

Ce TP vise √† mettre en pratique les concepts cl√©s li√©s aux Transformers, √† leur d√©ploiement et √† leur int√©gration dans une application simple.

## Technologies Utilis√©es

Voici les principales technologies et outils envisag√©s pour ce projet :

*   **Deep Learning & Backend :**
    *   ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
    *   ![Hugging Face Transformers](https://img.shields.io/badge/ü§ó%20Transformers-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black)
    *   ![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)
*   **API :**
    *   ![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi&logoColor=white)
*   **Frontend :**
    *   ![Vue.js](https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vue.js&logoColor=%234FC08D)
*   **Gestion de version & Environnement :**
    *   ![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white)
    *   ![Visual Studio Code](https://img.shields.io/badge/VS%20Code-007ACC?style=for-the-badge&logo=visualstudiocode&logoColor=white)

## Lancement du Projet

Cette section d√©crit les √©tapes pour configurer l'environnement et lancer les diff√©rents composants du projet.

### 1. Pr√©requis

*   Python 3.8+
*   pip (g√©n√©ralement inclus avec Python)

### 2. Installation des d√©pendances

Clonez d'abord le d√©p√¥t si ce n'est pas d√©j√† fait. Ensuite, ouvrez un terminal √† la racine du projet et ex√©cutez la commande suivante pour installer toutes les biblioth√®ques n√©cessaires list√©es dans `requirements.txt` :

```bash
pip install -r requirements.txt
```

### 3. Lancement du Backend (API FastAPI)

L'API FastAPI expose les mod√®les de deep learning. Pour la d√©marrer, ex√©cutez la commande suivante depuis la racine du projet :

```bash
python -m uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```
*   `python -m uvicorn` : Ex√©cute uvicorn en tant que module Python, ce qui est plus portable entre les environnements et les syst√®mes d\'exploitation.
*   `--reload` : Permet au serveur de red√©marrer automatiquement apr√®s des modifications du code. Utile en d√©veloppement.
*   `--host 0.0.0.0` : Rend l'API accessible depuis d'autres machines sur le r√©seau (et pas seulement en localhost).
*   `--port 8000` : Sp√©cifie le port d'√©coute (par d√©faut pour FastAPI avec uvicorn).

L'API sera alors accessible √† l'adresse `http://localhost:8000` (ou `http://<votre-ip-locale>:8000`).

### 4. Lancement du Frontend (Gradio)

L'interface utilisateur Gradio permet d'interagir avec l'API. Pour la d√©marrer, ex√©cutez la commande suivante depuis la racine du projet (en supposant que votre application Gradio se trouve dans `frontend/app.py`) :

```bash
python frontend/app.py
```

L'interface Gradio sera g√©n√©ralement accessible via une URL locale affich√©e dans le terminal (souvent `http://127.0.0.1:7860` ou similaire). Assurez-vous que le backend FastAPI est en cours d'ex√©cution pour que le frontend puisse communiquer avec les mod√®les.

## R√©ponses aux Questions du TP

### Partie 1 : Classification de sentiment

#### 4.1 Questions

**1. Exploration du Hub et s√©lection de mod√®les :**

*   **Identification de 3 mod√®les diff√©rents adapt√©s √† la classification de texte :**
    1.  `distilbert-base-uncased`
    2.  `bert-base-uncased`
    3.  `camembert-base`

*   **Analyse de leurs caract√©ristiques :**
    *   **`distilbert-base-uncased`**:
        *   **Taille**: Environ 66 millions de param√®tres.
        *   **Langue**: Anglais (non sensible √† la casse - "uncased").
        *   **Domaine d‚Äôentra√Ænement**: Principalement BookCorpus et Wikipedia anglais.
        *   **Architecture**: Encodeur seul (Transformer DistilBERT).
    *   **`bert-base-uncased`**:
        *   **Taille**: Environ 110 millions de param√®tres.
        *   **Langue**: Anglais (non sensible √† la casse - "uncased").
        *   **Domaine d‚Äôentra√Ænement**: Principalement BookCorpus et Wikipedia anglais.
        *   **Architecture**: Encodeur seul (Transformer BERT).
    *   **`camembert-base`**:
        *   **Taille**: Environ 110 millions de param√®tres.
        *   **Langue**: Fran√ßais.
        *   **Domaine d‚Äôentra√Ænement**: Corpus fran√ßais volumineux et vari√© (OSCAR).
        *   **Architecture**: Encodeur seul (Transformer RoBERTa-like).

*   **Justification du choix de 2 mod√®les pour votre exp√©rience :**

    Pour cette exp√©rience, nous choisirons **`distilbert-base-uncased`** et **`bert-base-uncased`**.
    *   **Comparaison directe**: Les deux mod√®les sont entra√Æn√©s sur des corpus anglais similaires (BookCorpus et Wikipedia anglais), ce qui permet une comparaison plus directe de leurs performances sur des t√¢ches en anglais.
    *   **Trade-off performance/ressources**: `distilbert-base-uncased` est une version distill√©e, plus petite et plus rapide de `bert-base-uncased`. Les comparer permettra d'analyser le compromis entre la taille/vitesse du mod√®le et ses performances. Ce crit√®re est important √©tant donn√© la contrainte d'utilisation sur un PC classique avec une RTX 3070 8GB VRAM.
    *   **Popularit√© et documentation**: Ce sont des mod√®les tr√®s populaires et largement document√©s, facilitant leur utilisation, le fine-tuning, et la recherche de solutions en cas de probl√®me.

*   **Comparaison des architectures sous-jacentes (encoder-only vs encoder-decoder) :**

    Les trois mod√®les identifi√©s (`distilbert-base-uncased`, `bert-base-uncased`, `camembert-base`) sont des mod√®les de type **encodeur seul (encoder-only)**.

    *   **Architecture encodeur seul**:
        *   Ces mod√®les sont constitu√©s d'une pile d'encodeurs Transformer. Leur r√¥le est de lire l'int√©gralit√© de la s√©quence d'entr√©e et de g√©n√©rer une repr√©sentation contextuelle riche de chaque token dans la s√©quence.
        *   Ils sont particuli√®rement bien adapt√©s aux t√¢ches de compr√©hension du langage naturel (NLU) comme la classification de texte (notre cas ici), la reconnaissance d'entit√©s nomm√©es, ou l'extraction de r√©ponses dans un contexte donn√©. Pour la classification, la repr√©sentation de sortie (souvent l'√©tat cach√© du token sp√©cial `[CLS]` ou une moyenne des √©tats cach√©s des tokens de la s√©quence) est ensuite utilis√©e comme entr√©e pour une couche de classification simple.

    *   **Diff√©rence avec les architectures encodeur-d√©codeur**:
        *   Les mod√®les **encodeur-d√©codeur** (exemples : T5, BART, MarianMT pour la traduction) poss√®dent deux principales composantes : un encodeur qui traite la s√©quence d'entr√©e pour la transformer en une repr√©sentation continue, et un d√©codeur qui utilise cette repr√©sentation pour g√©n√©rer une s√©quence de sortie token par token.
        *   Ils sont nativement con√ßus pour les t√¢ches de s√©quence √† s√©quence (seq2seq) telles que la traduction automatique, le r√©sum√© de texte, ou la g√©n√©ration de texte conditionnelle.
        *   Bien qu'un mod√®le encodeur-d√©codeur puisse √™tre adapt√© pour la classification (par exemple, en lui faisant g√©n√©rer le nom de la classe sous forme de texte : "positif", "n√©gatif"), les mod√®les encodeur seul sont g√©n√©ralement plus directs, plus l√©gers en termes de param√®tres pour une t√¢che √©quivalente, et souvent plus performants pour les t√¢ches de classification pure.

    *   **Pertinence pour la classification de texte**: Pour la classification de sentiment, l'objectif est de comprendre le sens global d'un texte pour lui assigner une √©tiquette. Une architecture encodeur seul est donc tout √† fait appropri√©e et constitue le choix standard et le plus efficace pour cette t√¢che.


### Partie 2 : G√©n√©ration de texte et question-r√©ponse

#### 5.1 Questions

**(a) S√©lection et comparaison de mod√®les g√©n√©ratifs :**

*   **Identification de 2 mod√®les adapt√©s √† la g√©n√©ration de texte sur le Hub :**

    Pour la g√©n√©ration de texte, nous allons consid√©rer les mod√®les suivants :
    1.  **`gpt2`** (ou sa version distill√©e `distilgpt2` pour une empreinte plus faible) : Un mod√®le auto-r√©gressif bas√© sur l'architecture Transformer de type d√©codeur seul.
    2.  **`t5-small`** : Un mod√®le s√©quence √† s√©quence (encodeur-d√©codeur) qui peut √™tre utilis√© pour la g√©n√©ration de texte conditionnelle.

*   **Comparaison de leurs approches : autoregressive vs seq2seq :**

    *   **Mod√®les Auto-r√©gressifs (Decoder-Only, ex: GPT-2)**:
        *   **Principe**: Ces mod√®les g√©n√®rent du texte un token √† la fois. Chaque nouveau token est pr√©dit en se basant sur la s√©quence des tokens pr√©c√©demment g√©n√©r√©s. Ils lisent la s√©quence de gauche √† droite (pour les langues occidentales) et pr√©disent le token suivant.
        *   **Architecture**: Ils utilisent typiquement uniquement la partie d√©codeur d'une architecture Transformer. Le m√©canisme d'attention leur permet de prendre en compte l'ensemble du contexte pr√©c√©dent lors de la g√©n√©ration de chaque nouveau token.
        *   **Cas d'usage**: Tr√®s efficaces pour la g√©n√©ration de texte libre (non conditionn√©e ou conditionn√©e par un prompt initial), la compl√©tion de texte, l'√©criture cr√©ative.
        *   **Exemple**: `gpt2` est un exemple canonique. Si on lui donne le prompt "Once upon a time, in a land far away,", il va pr√©dire le token suivant, puis le suivant, en se basant √† chaque fois sur tout ce qui a √©t√© g√©n√©r√© jusqu'alors.

    *   **Mod√®les S√©quence √† S√©quence (Encoder-Decoder, ex: T5-small)**:
        *   **Principe**: Ces mod√®les sont con√ßus pour transformer une s√©quence d'entr√©e en une s√©quence de sortie. L'encodeur traite la s√©quence d'entr√©e compl√®te pour en cr√©er une repr√©sentation (un √©tat cach√©). Le d√©codeur utilise ensuite cette repr√©sentation pour g√©n√©rer la s√©quence de sortie, token par token, de mani√®re auto-r√©gressive (similaire √† un mod√®le d√©codeur seul, mais conditionn√© par la sortie de l'encodeur).
        *   **Architecture**: Ils utilisent √† la fois un encodeur et un d√©codeur.
        *   **Cas d'usage**: Id√©aux pour les t√¢ches o√π la sortie est une transformation de l'entr√©e, comme la traduction automatique (entr√©e : phrase en langue A, sortie : phrase en langue B), le r√©sum√© de texte (entr√©e : texte long, sortie : r√©sum√© court), la r√©ponse √† des questions (entr√©e : question + contexte, sortie : r√©ponse). Pour la g√©n√©ration de texte plus "libre", on peut les utiliser en donnant un prompt √† l'encodeur et en laissant le d√©codeur g√©n√©rer la suite.
        *   **Exemple**: `t5-small` est pr√©-entra√Æn√© sur une multitude de t√¢ches en utilisant des pr√©fixes sp√©cifiques pour indiquer la t√¢che √† effectuer (ex: "translate English to French: ...", "summarize: ..."). Pour la g√©n√©ration de texte √† partir d'un prompt, on peut simplement donner le prompt comme entr√©e.

*   **Test des capacit√©s de g√©n√©ration avec diff√©rents prompts (Exemples conceptuels) :**

    *   **Prompt 1 (Cr√©atif)**: "Le dragon ouvrit un ≈ìil et dit √† la princesse :"
        *   **`gpt2` (attendu)**: Pourrait g√©n√©rer une suite narrative, par exemple : "... 'Votre qu√™te est noble, mais le chemin est sem√© d'emb√ªches. Cherchez l'oracle de la montagne interdite.'"
        *   **`t5-small` (attendu, si utilis√© pour la compl√©tion)**: Pourrait g√©n√©rer quelque chose de similaire, peut-√™tre plus concis ou factuel selon son entra√Ænement, par exemple : "... 'Bonjour.'"

    *   **Prompt 2 (Factuel/Question simple)**: "La capitale de la France est"
        *   **`gpt2` (attendu)**: Devrait g√©n√©rer "Paris." et potentiellement continuer avec des informations li√©es si on le laisse g√©n√©rer plus de tokens.
        *   **`t5-small` (attendu)**: Devrait √©galement g√©n√©rer "Paris." de mani√®re concise, car il est entra√Æn√© sur des t√¢ches de type question-r√©ponse.

    *   **Prompt 3 (Instruction simple)**: "√âcris un po√®me sur la lune."
        *   **`gpt2` (attendu)**: Pourrait g√©n√©rer quelques vers, dont la qualit√© po√©tique varierait. Exemple : "Astre p√¢le dans la nuit noire, / Tu veilles sur nos espoirs, / Silencieuse et lointaine, / Reine de la nuit sereine."
        *   **`t5-small` (attendu, si le prompt est bien formul√© pour une t√¢che de g√©n√©ration)**: Pourrait aussi tenter de g√©n√©rer un po√®me, bien que sa force r√©side plus dans la transformation de t√¢ches structur√©es. La qualit√© pourrait √™tre plus variable pour une t√¢che aussi ouverte sans fine-tuning sp√©cifique.

*   **Analyse de la qualit√© et de la coh√©rence des textes g√©n√©r√©s (Attendus) :**

    *   **`gpt2` / `distilgpt2`**:
        *   **Qualit√©**: Tendance √† produire un texte grammaticalement correct et souvent plausible localement. La coh√©rence sur de longues s√©quences peut parfois se d√©grader, avec des r√©p√©titions ou des d√©rives de sujet, surtout pour les versions plus petites comme `distilgpt2`.
        *   **Coh√©rence**: Bonne coh√©rence √† court et moyen terme. Pour des textes plus longs, un prompt bien formul√© et des techniques de d√©codage (comme le beam search, top-k/top-p sampling) peuvent aider √† maintenir la coh√©rence. Peut parfois g√©n√©rer des informations factuellement incorrectes (hallucinations) car il est optimis√© pour la plausibilit√© linguistique plut√¥t que la v√©racit√©.

    *   **`t5-small`**:
        *   **Qualit√©**: Lorsqu'utilis√© pour des t√¢ches pour lesquelles il a √©t√© explicitement entra√Æn√© (via les pr√©fixes), la qualit√© est g√©n√©ralement bonne et factuelle. Pour la g√©n√©ration de texte plus libre √† partir d'un prompt, la qualit√© peut √™tre plus h√©t√©rog√®ne sans fine-tuning sp√©cifique. Le texte est souvent grammaticalement correct.
        *   **Coh√©rence**: Bonne coh√©rence pour les t√¢ches structur√©es. Pour la g√©n√©ration libre, il peut √™tre plus enclin √† g√©n√©rer des phrases plus courtes ou √† s'arr√™ter plus t√¥t que GPT-2 s'il n'est pas certain de la suite. Sa nature encodeur-d√©codeur le rend bon pour suivre les instructions implicites du prompt (s'il est formul√© comme une t√¢che qu'il conna√Æt).
        *   **Sp√©cificit√©**: `t5-small` est un mod√®le plus petit de la famille T5, donc ses capacit√©s de g√©n√©ration seront moins impressionnantes que les versions plus grandes (T5-base, T5-large). Il est cependant plus g√©rable en termes de ressources.

    **Note**: Ces analyses sont bas√©es sur les caract√©ristiques g√©n√©rales des architectures et des mod√®les. Les r√©sultats r√©els d√©pendront fortement des prompts exacts, des param√®tres de g√©n√©ration (temp√©rature, top-k, etc.), et d'un √©ventuel fine-tuning.