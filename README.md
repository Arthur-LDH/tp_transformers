*# tp_transformers

## Contexte du TP

Ce projet s'inscrit dans le cadre d'un Travail Pratique (TP) ax√© sur l'exploration et l'application des mod√®les de Transformers. L'objectif principal est de d√©velopper une solution de bout en bout, comprenant typiquement :
*   La s√©lection et l'exp√©rimentation avec un mod√®le de Transformer adapt√© √† une t√¢che sp√©cifique (par exemple, classification de texte, g√©n√©ration de texte, question-r√©ponse).
*   Le fine-tuning (ou l'entra√Ænement) du mod√®le sur un jeu de donn√©es pertinent.
*   L'exposition des fonctionnalit√©s du mod√®le entra√Æn√© via une API RESTful.
*   La cr√©ation d'une interface utilisateur simple (frontend) pour interagir avec le mod√®le via l'API.

Ce TP vise √† mettre en pratique les concepts cl√©s li√©s aux Transformers, √† leur d√©ploiement et √† leur int√©gration dans une application simple.

## Technologies Utilis√©es

Voici les principales technologies et outils envisag√©s pour ce projet :

*   **Deep Learning & Backend :**
    *   ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
    *   ![Hugging Face Transformers](https://img.shields.io/badge/ü§ó%20Transformers-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black)
    *   ![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)
*   **API :**
    *   ![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi&logoColor=white)
*   **Frontend :**
    *   ![Vue.js](https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vue.js&logoColor=%234FC08D)
*   **Gestion de version & Environnement :**
    *   ![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white)
    *   ![Visual Studio Code](https://img.shields.io/badge/VS%20Code-007ACC?style=for-the-badge&logo=visualstudiocode&logoColor=white)

## Recherche de Mod√®les sur Hugging Face Hub

Lors de la recherche de mod√®les de Transformers adapt√©s pour ce TP, plusieurs facteurs ont √©t√© pris en compte, notamment la performance, la taille du mod√®le, et la compatibilit√© avec du mat√©riel courant (ex: une carte graphique RTX 3070 avec 8GB de VRAM).

### Mod√®les Envisag√©s

*   **`distilbert-base-uncased`**:
    *   **Description**: Il s'agit d'une version distill√©e de BERT (Bidirectional Encoder Representations from Transformers). La distillation est une technique qui permet de r√©duire la taille d'un mod√®le tout en essayant de conserver une grande partie de ses performances. `distilbert-base-uncased` est significativement plus petit et plus rapide que `bert-base-uncased`.
    *   **Avantages**:
        *   **Taille r√©duite**: Moins de param√®tres, ce qui signifie moins d'utilisation de m√©moire VRAM et un chargement plus rapide.
        *   **Inf√©rence rapide**: Convient bien pour des applications n√©cessitant des r√©ponses rapides.
        *   **Bonnes performances**: Bien qu'√©tant plus petit, il maintient des performances respectables sur de nombreuses t√¢ches de compr√©hension du langage naturel (NLU).
        *   **Compatibilit√©**: Fonctionne bien sur des GPU avec une VRAM mod√©r√©e comme une RTX 3070 8GB.
    *   **Cas d'usage typiques**: Classification de texte, analyse de sentiment, question-r√©ponse (apr√®s fine-tuning).
    *   **Remarques**: Le suffixe "uncased" signifie que le mod√®le ne fait pas de distinction entre les majuscules et les minuscules.

### Consid√©rations pour le choix d'un mod√®le sur une RTX 3070 (8GB VRAM)

*   **Taille du mod√®le (Nombre de param√®tres)**: Les mod√®les plus grands (ex: BERT-large, GPT-2 medium/large) peuvent d√©passer la capacit√© de 8GB de VRAM, surtout pendant l'entra√Ænement ou si la taille du batch est importante. Les mod√®les "base" ou "distilled" sont g√©n√©ralement plus adapt√©s.
*   **Quantification**: Certains mod√®les sont disponibles en versions quantifi√©es (ex: INT8). La quantification r√©duit la pr√©cision des poids du mod√®le (ex: de FP32 √† INT8), ce qui diminue l'utilisation de la m√©moire et peut acc√©l√©rer l'inf√©rence, parfois avec une l√©g√®re perte de performance.
*   **Pr√©cision (FP16 vs FP32)**: L'utilisation de la pr√©cision mixte (FP16) pendant l'entra√Ænement ou l'inf√©rence peut r√©duire de moiti√© l'utilisation de la VRAM par rapport √† la pleine pr√©cision (FP32), tout en acc√©l√©rant les calculs sur les GPU compatibles.
*   **Taille du batch**: Lors de l'entra√Ænement ou de l'inf√©rence par batch, une taille de batch plus petite consomme moins de VRAM. Il faut trouver un √©quilibre entre la taille du batch et la stabilit√©/vitesse de l'entra√Ænement.
*   **Longueur de s√©quence**: Les Transformers ont une consommation m√©moire qui augmente quadratiquement avec la longueur de la s√©quence d'entr√©e. Des s√©quences plus courtes n√©cessitent moins de m√©moire.

### Conclusion Pr√©liminaire

Pour ce TP, commencer avec des mod√®les comme `distilbert-base-uncased` ou d'autres mod√®les BERT-like de taille "base" semble √™tre une approche judicieuse. Ils offrent un bon compromis entre performance et ressources n√©cessaires. Il sera toujours possible d'explorer des mod√®les plus grands ou des techniques d'optimisation (quantification, pruning) si les besoins du projet √©voluent et si les ressources le permettent.

Il est recommand√© de consulter r√©guli√®rement le [Hugging Face Hub](https://huggingface.co/models) pour d√©couvrir de nouveaux mod√®les ou des versions fine-tun√©es sp√©cifiques √† certaines t√¢ches.